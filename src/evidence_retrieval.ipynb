{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the training, validation data, and evidence set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/train-claims.json')\n",
    "raw_train_data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/dev-claims.json')\n",
    "raw_val_data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/evidence.json')\n",
    "evidence_data = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "for evidence_id, evidence in evidence_data.items():\n",
    "    evidence_data[evidence_id] = evidence.lower()\n",
    "\n",
    "def preprocess_data(raw_data, evidence_set):\n",
    "    # Map labels into numbers\n",
    "    label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "    processed_data = []\n",
    "    for claim_id, claim_info in raw_data.items():\n",
    "        \n",
    "        # Lower all the words in both claim and evidences\n",
    "        claim_text = claim_info['claim_text'].lower()\n",
    "        evidences = [evidence_set[evidence_id] for evidence_id in claim_info['evidences'][:5]]  # Limit to max 5 evidence\n",
    "        label = label_map[claim_info['claim_label']]\n",
    "        for evidence in evidences:\n",
    "            processed_data.append((claim_text, evidence, label))\n",
    "    return processed_data\n",
    "\n",
    "# Load and preprocess your training and validation datasets\n",
    "train_data = preprocess_data(raw_train_data, evidence_data)\n",
    "val_data = preprocess_data(raw_val_data, evidence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRTrainDataset(Dataset):\n",
    "    def __init__(self, train_data, claim_max_len, evidence_max_len):\n",
    "        self.data = train_data\n",
    "        self.claim_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        self.evidence_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        self.claim_max_len = claim_max_len\n",
    "        self.evidence_max_len = evidence_max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        claim = self.data[index][0]\n",
    "        evidence = self.data[index][1]\n",
    "\n",
    "        # Tokenize the claim and evidence\n",
    "        claim_tokens = self.claim_tokenizer.tokenize(claim)\n",
    "        claim_tokens = ['[CLS]'] + claim_tokens + ['[SEP]']\n",
    "        evidence_tokens = self.evidence_tokenizer.tokenize(evidence)\n",
    "        evidence_tokens = ['[CLS]'] + evidence_tokens + ['[SEP]']\n",
    "\n",
    "        # Pad to sequence to the same length if the length is less than max length\n",
    "        if len(claim_tokens) < self.claim_max_len:\n",
    "            claim_tokens = claim_tokens + ['[PAD]' for _ in range(self.claim_max_len - len(claim_tokens))] #Padding sentences\n",
    "        else:\n",
    "            claim_tokens = claim_tokens[:self.claim_max_len-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        if len(evidence_tokens) < self.evidence_max_len:\n",
    "            evidence_tokens = evidence_tokens + ['[PAD]' for _ in range(self.evidence_max_len - len(evidence_tokens))] #Padding sentences\n",
    "        else:\n",
    "            evidence_tokens = evidence_tokens[:self.evidence_max_len-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        claim_tokens_ids = self.claim_tokenizer.convert_tokens_to_ids(claim_tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        claim_tokens_ids_tensor = torch.tensor(claim_tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        evidence_tokens_ids = self.evidence_tokenizer.convert_tokens_to_ids(evidence_tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        evidence_tokens_ids_tensor = torch.tensor(evidence_tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        claim_attn_mask = (claim_tokens_ids_tensor != 0).long()\n",
    "        evidence_attn_mask = (evidence_tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return claim_tokens_ids_tensor, evidence_tokens_ids_tensor, claim_attn_mask, evidence_attn_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
