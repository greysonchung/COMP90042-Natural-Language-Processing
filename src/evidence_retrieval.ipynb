{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the training, validation data, and evidence set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/train-claims.json')\n",
    "raw_train_data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/dev-claims.json')\n",
    "raw_val_data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/evidence.json')\n",
    "evidence_data = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "for evidence_id, evidence in evidence_data.items():\n",
    "    evidence_data[evidence_id] = evidence.lower()\n",
    "\n",
    "def preprocess_data(raw_data, evidence_set):\n",
    "    # Map labels into numbers\n",
    "    label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "    processed_data = []\n",
    "    for claim_id, claim_info in raw_data.items():\n",
    "        \n",
    "        # Lower all the words in both claim and evidences\n",
    "        claim_text = claim_info['claim_text'].lower()\n",
    "        evidences = [evidence_set[evidence_id] for evidence_id in claim_info['evidences'][:5]]  # Limit to max 5 evidence\n",
    "        evidences = \" [SEP] \".join(evidences)\n",
    "        label = label_map[claim_info['claim_label']]\n",
    "        processed_data.append((claim_text, evidences, label, claim_info['evidences']))\n",
    "    return processed_data\n",
    "\n",
    "# Load and preprocess your training and validation datasets\n",
    "train_data = preprocess_data(raw_train_data, evidence_data)\n",
    "val_data = preprocess_data(raw_val_data, evidence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/config.json\n",
      "Model config DPRConfig {\n",
      "  \"_name_or_path\": \"facebook/dpr-question_encoder-single-nq-base\",\n",
      "  \"architectures\": [\n",
      "    \"DPRQuestionEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/config.json\n",
      "Model config DPRConfig {\n",
      "  \"_name_or_path\": \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
      "  \"architectures\": [\n",
      "    \"DPRContextEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Initialise the tokenizers\n",
    "\n",
    "claim_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "evidence_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRTrainDataset(Dataset):\n",
    "    def __init__(self, train_data):\n",
    "        self.data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        claims = self.data[index][0]\n",
    "        evidences = self.data[index][1]\n",
    "        evidence_ids = self.data[index][3]\n",
    "\n",
    "        return claims, evidences, evidence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(claims, evidence_ids):\n",
    "    '''\n",
    "    This function performs negative sampling for a batch of claims\n",
    "    '''\n",
    "    batch_evidence_set = set([ids for sublist in evidence_ids for ids in sublist])\n",
    "    negative_evidences = []\n",
    "    \n",
    "    for i in range(len(claims)):\n",
    "        negative_candidates = list(batch_evidence_set - set(evidence_ids[i]))\n",
    "        sample_size = len(evidence_ids[i])\n",
    "        sample_size = min(sample_size, len(negative_candidates))  # Add this line\n",
    "        sampled_ids = random.sample(negative_candidates, sample_size)\n",
    "\n",
    "        # Retrieve the actual evidence base on evidence ids\n",
    "        sampled_evidence = [evidence_data[evidence_id] for evidence_id in sampled_ids]\n",
    "        sampled_evidence = \" [SEP] \".join(sampled_evidence)\n",
    "        negative_evidences.append(sampled_evidence)\n",
    "\n",
    "    return negative_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining collate function to process and combine samples from the dataset into a single batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    claims, evidences, evidence_ids = zip(*batch)\n",
    "\n",
    "    negative_evidences = negative_sampling(claims, evidence_ids)\n",
    "    \n",
    "    # Tokenize the claim and evidence\n",
    "    claim_tokens = claim_tokenizer(claims, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    positive_evidence_tokens = evidence_tokenizer(evidences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    negative_evidence_tokens = evidence_tokenizer(negative_evidences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"claim_inputs\": claim_tokens,\n",
    "        \"positive_evidence_inputs\": positive_evidence_tokens,\n",
    "        \"negative_evidence_inputs\": negative_evidence_tokens,\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/config.json\n",
      "Model config DPRConfig {\n",
      "  \"architectures\": [\n",
      "    \"DPRQuestionEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DPRQuestionEncoder were initialized from the model checkpoint at facebook/dpr-question_encoder-single-nq-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DPRQuestionEncoder for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/config.json\n",
      "Model config DPRConfig {\n",
      "  \"architectures\": [\n",
      "    \"DPRContextEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DPRContextEncoder were initialized from the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DPRContextEncoder for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoders\n",
    "\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m LEARNING_RATE \u001b[39m=\u001b[39m \u001b[39m1e-5\u001b[39m\n\u001b[0;32m----> 3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(\u001b[39mlist\u001b[39m(question_encoder\u001b[39m.\u001b[39mparameters()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(context_encoder\u001b[39m.\u001b[39mparameters()), lr\u001b[39m=\u001b[39mLEARNING_RATE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 1e-5\n",
    "optimizer = torch.optim.AdamW(list(question_encoder.parameters()) + list(context_encoder.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        question_encoder.train()\n",
    "        context_encoder.train()\n",
    "\n",
    "        progress_bar = tqdm(data_loader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            claim_inputs = {k: v.to(device) for k, v in batch[\"claim_inputs\"].items()}\n",
    "            positive_evidence_inputs = {k: v.to(device) for k, v in batch[\"positive_evidence_inputs\"].items()}\n",
    "            negative_evidence_inputs = {k: v.to(device) for k, v in batch[\"negative_evidence_inputs\"].items()}\n",
    "\n",
    "            # Encodes the inputs using encoders\n",
    "            claim_outputs = question_encoder(**claim_inputs)[\"pooler_output\"]\n",
    "            positive_evidence_outputs = context_encoder(**positive_evidence_inputs)[\"pooler_output\"]\n",
    "            negative_evidence_outputs = context_encoder(**negative_evidence_inputs)[\"pooler_output\"]\n",
    "\n",
    "            # Compute similarity scores\n",
    "            positive_similarity_scores = torch.matmul(claim_outputs, positive_evidence_outputs.T)\n",
    "            negative_similarity_scores = torch.matmul(claim_outputs, negative_evidence_outputs.T)\n",
    "\n",
    "            # Concatenate positive and negative similarity scores\n",
    "            all_similarity_scores = torch.cat([positive_similarity_scores, negative_similarity_scores], dim=1)\n",
    "\n",
    "            # Compute probabilities using softmax\n",
    "            probabilities = F.softmax(all_similarity_scores, dim=1)\n",
    "\n",
    "            # Compute the negative log likelihood loss using the positive similarity scores' probabilities\n",
    "            positive_indices = torch.arange(probabilities.size(0), dtype=torch.long, device=device)\n",
    "            loss = F.nll_loss(torch.log(probabilities), positive_indices)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "        \n",
    "    print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DPRTrainDataset(train_data)\n",
    "train_loader = DataLoader(train_ds, batch_size=6, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 205/205 [11:24<00:00,  3.34s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.29675430059432983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 205/205 [11:12<00:00,  3.28s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.14043737947940826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 205/205 [11:28<00:00,  3.36s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.008969031274318695\n",
      "Fine-tuning complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model locally\n",
    "question_encoder_save_path = \"tuned/question_encoder\"\n",
    "context_encoder_save_path = \"tuned/context_encoder\"\n",
    "# question_encoder.save_pretrained(question_encoder_save_path)\n",
    "# context_encoder.save_pretrained(context_encoder_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the evidence from the evidence dataset using the tuned ContextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file tuned/question_encoder/config.json\n",
      "Model config DPRConfig {\n",
      "  \"_name_or_path\": \"facebook/dpr-question_encoder-single-nq-base\",\n",
      "  \"architectures\": [\n",
      "    \"DPRQuestionEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file tuned/question_encoder/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DPRQuestionEncoder.\n",
      "\n",
      "All the weights of DPRQuestionEncoder were initialized from the model checkpoint at tuned/question_encoder.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DPRQuestionEncoder for predictions without further training.\n",
      "loading configuration file tuned/context_encoder/config.json\n",
      "Model config DPRConfig {\n",
      "  \"_name_or_path\": \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
      "  \"architectures\": [\n",
      "    \"DPRContextEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file tuned/context_encoder/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DPRContextEncoder.\n",
      "\n",
      "All the weights of DPRContextEncoder were initialized from the model checkpoint at tuned/context_encoder.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DPRContextEncoder for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned models\n",
    "fine_tuned_question_encoder = DPRQuestionEncoder.from_pretrained(question_encoder_save_path)\n",
    "fine_tuned_context_encoder = DPRContextEncoder.from_pretrained(context_encoder_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the all text into a single list\n",
    "evidence_texts = list(evidence_data.values())\n",
    "tokenized_evidence = evidence_tokenizer(evidence_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m end_index \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m((i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size, \u001b[39mlen\u001b[39m(evidence_items))\n\u001b[1;32m     28\u001b[0m batch \u001b[39m=\u001b[39m evidence_items[start_index:end_index]\n\u001b[0;32m---> 29\u001b[0m encode_and_save_evidence_batch(batch, output_filename)\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mencode_and_save_evidence_batch\u001b[0;34m(batch, filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m evidence_id, evidence_text \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m      6\u001b[0m     inputs \u001b[39m=\u001b[39m evidence_tokenizer(evidence_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[39m=\u001b[39m fine_tuned_context_encoder(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39mto(device))[\u001b[39m\"\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m     encoded_batch[evidence_id] \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode evidence by batch to prevent memory overflow issue\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def encode_and_save_evidence_batch(batch, filename):\n",
    "    encoded_batch = {}\n",
    "    for evidence_id, evidence_text in batch:\n",
    "        inputs = evidence_tokenizer(evidence_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = fine_tuned_context_encoder(**inputs.to(device))[\"pooler_output\"]\n",
    "        encoded_batch[evidence_id] = outputs.cpu().detach().numpy().tolist()\n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        for evidence_id, embedding in encoded_batch.items():\n",
    "            f.write(json.dumps({evidence_id: embedding}))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "batch_size = 100\n",
    "evidence_items = list(evidence_data.items())\n",
    "num_batches = len(evidence_items) // batch_size + int(len(evidence_items) % batch_size > 0)\n",
    "\n",
    "output_filename = \"encoded_evidence.jsonl\"\n",
    "\n",
    "# Clear the file content before writing the encoded evidence\n",
    "with open(output_filename, \"w\") as f:\n",
    "    pass\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_index = i * batch_size\n",
    "    end_index = min((i + 1) * batch_size, len(evidence_items))\n",
    "    batch = evidence_items[start_index:end_index]\n",
    "    encode_and_save_evidence_batch(batch, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenized_evidence = {k: v.to(device) for k, v in tokenized_evidence.items()}\n",
    "with torch.no_grad():\n",
    "    encoded_evidence = fine_tuned_context_encoder(**tokenized_evidence)[\"pooler_output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
