{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the training, validation data, and evidence set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/train-claims.json')\n",
    "raw_train_data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/dev-claims.json')\n",
    "raw_val_data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/evidence.json')\n",
    "evidence_data = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "for evidence_id, evidence in evidence_data.items():\n",
    "    evidence_data[evidence_id] = evidence.lower()\n",
    "\n",
    "def preprocess_data(raw_data, evidence_set):\n",
    "    # Map labels into numbers\n",
    "    label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "    processed_data = []\n",
    "    for claim_id, claim_info in raw_data.items():\n",
    "        \n",
    "        # Lower all the words in both claim and evidences\n",
    "        claim_text = claim_info['claim_text'].lower()\n",
    "        evidences = [evidence_set[evidence_id] for evidence_id in claim_info['evidences'][:5]]  # Limit to max 5 evidence\n",
    "        evidences = \" [SEP] \".join(evidences)\n",
    "        label = label_map[claim_info['claim_label']]\n",
    "        processed_data.append((claim_text, evidences, label, claim_info['evidences']))\n",
    "    return processed_data\n",
    "\n",
    "# Load and preprocess your training and validation datasets\n",
    "train_data = preprocess_data(raw_train_data, evidence_data)\n",
    "val_data = preprocess_data(raw_val_data, evidence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/config.json\n",
      "Model config DPRConfig {\n",
      "  \"_name_or_path\": \"facebook/dpr-question_encoder-single-nq-base\",\n",
      "  \"architectures\": [\n",
      "    \"DPRQuestionEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/config.json\n",
      "Model config DPRConfig {\n",
      "  \"_name_or_path\": \"facebook/dpr-ctx_encoder-single-nq-base\",\n",
      "  \"architectures\": [\n",
      "    \"DPRContextEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Initialise the tokenizers\n",
    "\n",
    "claim_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "evidence_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRTrainDataset(Dataset):\n",
    "    def __init__(self, train_data, claim_max_len, evidence_max_len):\n",
    "        self.data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        claims = self.data[index][0]\n",
    "        evidences = self.data[index][1]\n",
    "        evidence_ids = self.data[index][3]\n",
    "\n",
    "        return claims, evidences, evidence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(claims, evidence_ids):\n",
    "    '''\n",
    "    This function performs negative sampling for a batch of claims\n",
    "    '''\n",
    "    batch_evidence_set = set([ids for ids in evidence_ids])\n",
    "    negative_evidences = []\n",
    "    \n",
    "    for i in range(len(claims)):\n",
    "        negative_candidates = batch_evidence_set - set(evidence_ids[i])\n",
    "        sample_size = len(evidence_ids[i])\n",
    "        sampled_ids = random.sample(negative_candidates, sample_size)\n",
    "\n",
    "        # Retrieve the actual evidence base on evidence ids\n",
    "        sampled_evidence = [evidence_data[evidence_id] for evidence_id in sampled_ids]\n",
    "        sampled_evidence = \" [SEP] \".join(sampled_evidence)\n",
    "        negative_evidences.append(sampled_evidence)\n",
    "\n",
    "    return negative_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining collate function to process and combine samples from the dataset into a single batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    claims, evidences, evidence_ids = zip(*batch)\n",
    "\n",
    "    negative_evidences = negative_sampling(claims, evidence_ids)\n",
    "    \n",
    "    # Tokenize the claim and evidence\n",
    "    claim_tokens = claim_tokenizer(claims, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    positive_evidence_tokens = evidence_tokenizer(evidences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    negative_evidence_tokens = evidence_tokenizer(negative_evidences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"claim_inputs\": claim_tokens,\n",
    "        \"positive_evidence_inputs\": positive_evidence_tokens,\n",
    "        \"negative_evidence_inputs\": negative_evidence_tokens,\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/config.json\n",
      "Model config DPRConfig {\n",
      "  \"architectures\": [\n",
      "    \"DPRQuestionEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-question_encoder-single-nq-base/snapshots/d04a52f6d2f96c60117a925e8c24c4043a75f265/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DPRQuestionEncoder were initialized from the model checkpoint at facebook/dpr-question_encoder-single-nq-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DPRQuestionEncoder for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/config.json\n",
      "Model config DPRConfig {\n",
      "  \"architectures\": [\n",
      "    \"DPRContextEncoder\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"dpr\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"projection_dim\": 0,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/greysonchung/.cache/huggingface/hub/models--facebook--dpr-ctx_encoder-single-nq-base/snapshots/bb21a3c2b1656d60c6a8e920283bc40dabddadb8/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DPRContextEncoder were initialized from the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DPRContextEncoder for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoders\n",
    "\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 1e-5\n",
    "optimizer = torch.optim.AdamW(list(question_encoder.parameters()) + \n",
    "                              list(context_encoder.parameters), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, data_loader, num_epochs, learning_rate):\n",
    "    for epoch in range(num_epochs):\n",
    "        question_encoder.train()\n",
    "        context_encoder.train()\n",
    "\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            claim_inputs = {k: v.to(device) for k, v in batch[\"claim_inputs\"].items()}\n",
    "            positive_evidence_inputs = {k: v.to(device) for k, v in batch[\"positive_evidence_inputs\"].items()}\n",
    "            negative_evidence_inputs = {k: v.to(device) for k, v in batch[\"negative_evidence_inputs\"].items()}\n",
    "\n",
    "            # Encodes the inputs using encoders\n",
    "            claim_outputs = question_encoder(**claim_inputs)[\"pooler_output\"]\n",
    "            positive_evidence_outputs = context_encoder(**positive_evidence_inputs)[\"pooler_output\"]\n",
    "            negative_evidence_outputs = context_encoder(**negative_evidence_inputs)[\"pooler_output\"]\n",
    "\n",
    "            # Compute similarity scores\n",
    "            positive_similarity_scores = torch.matmul(claim_outputs, positive_evidence_outputs.T)\n",
    "            negative_similarity_scores = torch.matmul(claim_outputs, negative_evidence_outputs.T)\n",
    "\n",
    "            # Concatenate positive and negative similarity scores\n",
    "            all_similarity_scores = torch.cat([positive_similarity_scores, negative_similarity_scores], dim=1)\n",
    "\n",
    "            # Compute probabilities using softmax\n",
    "            probabilities = F.softmax(all_similarity_scores, dim=1)\n",
    "\n",
    "            # Compute the negative log likelihood loss using the positive similarity scores' probabilities\n",
    "            positive_indices = torch.arange(probabilities.size(0), dtype=torch.long, device=device)\n",
    "            loss = F.nll_loss(torch.log(probabilities), positive_indices)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "        \n",
    "    print(\"Fine-tuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
